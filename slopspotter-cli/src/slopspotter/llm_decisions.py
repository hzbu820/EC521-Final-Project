"""Tools for testing LLM decision trees."""

import re

import networkx as nx
import torch
from transformers import (
    PreTrainedModel,
    PreTrainedTokenizer,
)
from transformers.utils.logging import disable_progress_bar

PACKAGE_INPUT_TEMPLATE = "Here is the name of a {0} package: `{1}"

BACKTICK_REGEX = re.compile("(.*)`(.*)`")

disable_progress_bar()


def package_from_node_text(llm_node_text: str):
    """Get the package from a decision tree node's text."""
    match = BACKTICK_REGEX.match(llm_node_text)
    if match is not None:
        return match.groups(0)[1]
    else:
        return None


def reset_control_codes(token: str) -> str:
    """Remove the ASCII control codes inside of a token.

    Some LLMs (including GPT-2) use characters from 0x100 to 0x120 as
    alternatives for ASCII characters 0x00 to 0x20.

    See also https://en.wikipedia.org/wiki/%C4%A0
    """
    modified_token = ""
    for character in token:
        if 0x100 <= ord(character) <= 0x120:
            modified_token += chr(ord(character) - 0x100)
        else:
            modified_token += character

    return modified_token


def topk_token_probabilities(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    input_text: str,
    k: int = 10,
) -> tuple[torch.Tensor, torch.Tensor]:
    r"""Get the K most probable tokens & probabilities the model would select.

    See also `torch.topk`.

    Usage:
        >>> from transformers import AutoModelForCausalLM, AutoTokenizer
        >>> model = AutoModelForCausalLM.from_pretrained(
        ...    "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
        ... )
        >>> tokenizer = AutoTokenizer.from_pretrained(
        ...    "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
        ... )
        >>> input_text = "Here is a list of Python packages.\n\n- "
        >>> topk_token_probabilities(model, tokenizer, input_text)

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        input_text: text previously generated by LLM / inputted by user
        k: the k in "top-k"

    Returns:
        (topk_values, topk_indices): tuple of torch tensors
    """
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits

    last_token_logits = logits[0, -1, :]
    probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)

    topk_values, topk_indices = torch.topk(probabilities, k=k)
    return topk_values, topk_indices


def token_probability(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    input_ids: torch.Tensor,
    token_id: int,
):
    """Calculate the LLM's probability for choosing a specific token ID next.

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        input_ids: tokens previously generated by LLM / inputted by user
        token_id: next token ID
    """
    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits

    last_token_logits = logits[0, -1, :]
    probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)

    return probabilities[token_id].item()


def token_by_token_probability(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    input_text: str,
    output_text: str,
) -> torch.Tensor:
    """Calculate the LLM's token-by-token probability for the text in `token_text`.

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        input_text: text previously generated by LLM
        output_text: text to be generated by LLM
    """
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)
    token_ids = tokenizer.encode(output_text, return_tensors="pt").to(model.device)
    probabilities = torch.empty(token_ids.shape[1]).to(model.device)
    for token_idx, token_id in enumerate(token_ids[0, :]):
        probabilities[token_idx] = token_probability(
            model,
            tokenizer,
            torch.hstack((input_ids, token_ids[:, 0:token_idx])),
            token_id,
        )
    return probabilities


def balanced_tree_order(r: int, h: int) -> int:
    """Calculate the order (total number of nodes) in a balanced tree.

    See also `networkx.balanced_tree`.

    Args:
        r: Branching factor of the tree; each node will have `r` children.
        h: Height of the tree.

    Returns:
        order: number of nodes in the balanced tree.

    See also https://stackoverflow.com/a/7842866
    """
    return (r ** (h + 1) - 1) // (r - 1)


def token_decision_tree(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    input_text: str,
    k: int = 3,
    max_depth: int = 2,
    stop_strings: tuple[str, ...] | None = None,
) -> nx.DiGraph:
    """Calculate the LLM's top-k token decision tree.

    This function runs the given LLM, fully exploring the `k ** max_depth` most
    probable outcomes that the LLM could output based on the given input text.

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        input_text: text previously generated by LLM / inputted by user
        k: the k in "top-k"
        max_depth: decision tree depth
        stop_strings: strings that stop token branching if inside of a token

    Raises:
        ValueError: if `max_depth` is not an integer greater than 0

    Returns:
        decision_tree: NetworkX digraph decision tree
    """
    if not max_depth > 0:
        raise ValueError("max depth must be an integer greater than 0")
    if stop_strings is None:
        stop_strings = tuple()

    decision_tree = nx.DiGraph()

    input_ids = tokenizer.encode(input_text, return_tensors="np")[-1]
    last_input_id = input_ids[-1]

    decision_tree.add_node(
        0,
        token_id=int(last_input_id),
        token=reset_control_codes(tokenizer.decode(last_input_id)),
        depth=0,
        input_text=input_text,
        expected=True,
    )

    # We don't need to run top-k token calculations for nodes in the last layer
    for node_id in range(balanced_tree_order(k, max_depth - 1)):
        if not decision_tree.has_node(node_id):
            # Skip over nodes that don't exist from pruning
            continue

        current_depth = decision_tree.nodes[node_id]["depth"]

        if current_depth != 0 and any(
            [s in decision_tree.nodes[node_id]["token"] for s in stop_strings]
        ):
            # Don't propagate nodes that match a stop string
            continue

        node_input_text = input_text
        if current_depth != 0:
            traversal = nx.shortest_path(decision_tree, 0, node_id)
            node_input_text += tokenizer.decode(
                [decision_tree.nodes[n]["token_id"] for n in traversal[1:]]
            )

        topk_token_probs, topk_token_ids = topk_token_probabilities(
            model, tokenizer, node_input_text, k=k
        )
        topk_tokens = tokenizer.convert_ids_to_tokens(topk_token_ids)

        current_order = decision_tree.order()
        new_node_ids = range(current_order, current_order + k)

        for k_i, new_node_id in zip(range(k), new_node_ids, strict=True):
            decision_tree.add_node(
                new_node_id,
                depth=current_depth + 1,
                token_id=topk_token_ids[k_i].item(),
                token=reset_control_codes(topk_tokens[k_i]),
                expected=False,
            )
            decision_tree.add_edge(
                node_id,
                new_node_id,
                probability=topk_token_probs[k_i].item(),
                expected=False,
            )

    return decision_tree


def predict_hallucinated_packages(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    language: str | None,
    package: str | None,
    k: int = 3,
    max_depth: int = 5,
) -> nx.DiGraph:
    """Predict alternative hallucinated packages using the given model and tokenizer.

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        language: the name of a programming language
        package: the name of a package in that programming language
        k: the k in "top-k"
        max_depth: decision tree depth
    """
    if not max_depth >= 1:
        raise ValueError("max depth must be an integer greater than 1")

    # language & package default values
    language = language.title() if language else "programming language"
    package_tokens = tokenizer.tokenize(package) if package else [""]

    # initialize input text
    input_text = PACKAGE_INPUT_TEMPLATE.format(language, package_tokens[0])

    decision_tree = token_decision_tree(
        model,
        tokenizer,
        input_text,
        k,
        max_depth,
        stop_strings=("`", "\t", "\n", "\v", "\f", "\r"),
    )

    if len(package_tokens) <= 1:
        return decision_tree

    return add_expected_output(
        model, tokenizer, decision_tree, input_text, package_tokens
    )


def add_expected_output(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    decision_tree: nx.DiGraph,
    input_text: str,
    expected_tokens: list[str],
) -> nx.DiGraph:
    """Add the expected tokens to a given decision tree.

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        decision_tree: LLM top-k token decision tree
        input_text: text previously generated by LLM
        expected_tokens: list of expected tokens

    Returns:
        new modified decision tree
    """
    rest_probs = token_by_token_probability(
        model, tokenizer, input_text, "".join(expected_tokens[1:])
    )
    rest_token_ids = tokenizer.convert_tokens_to_ids(expected_tokens[1:])

    current_node_id = 0
    for prob, token, token_id in zip(
        rest_probs, expected_tokens[1:], rest_token_ids, strict=True
    ):
        current_depth = decision_tree.nodes[current_node_id]["depth"]
        successor_token_ids = [
            decision_tree.nodes[successor]["token_id"]
            for successor in decision_tree.successors(current_node_id)
        ]
        if token_id not in successor_token_ids:
            new_node_id = decision_tree.order()
            decision_tree.add_node(
                new_node_id,
                depth=current_depth + 1,
                token_id=token_id,
                token=reset_control_codes(token),
                expected=True,
            )
            decision_tree.add_edge(
                current_node_id,
                new_node_id,
                probability=prob.item(),
                expected=True,
            )
            current_node_id = new_node_id
        else:
            current_node_id = successor_token_ids.index(token_id)

    return decision_tree


def packages_from_token_decision_tree(decision_tree: nx.DiGraph) -> set:
    """Obtain the set of hallucinated package names from an LLM's token decision tree.

    Args:
        decision_tree: LLM top-k token decision tree
    """
    input_text = decision_tree.nodes[0]["input_text"]
    package_names = set()
    for node_id in decision_tree.nodes:
        if len(list(decision_tree.successors(node_id))) > 0:
            continue

        traversal = nx.shortest_path(decision_tree, 0, node_id)
        node_text = input_text + "".join(
            [decision_tree.nodes[n]["token"] for n in traversal[1:]]
        )
        package_names.add(package_from_node_text(node_text))
    return package_names


def package_in_vocabulary(tokenizer: PreTrainedTokenizer, package: str) -> bool:
    """Determine whether the package's name is already in the tokenizer.

    We can assume that short and/or common package names like `numpy` are more
    trustworthy if they already exist in the tokenizer's vocabulary.

    Note that this should only be a worthwhile metric  the package actually
    is written in that language (e.g., `numpy` doesn't exist in JavaScript).
    """
    return package in tokenizer.get_vocab()
