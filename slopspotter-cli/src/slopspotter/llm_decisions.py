"""Tools for testing LLM decision trees."""

from typing import Literal

import networkx as nx
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.utils.logging import disable_progress_bar

disable_progress_bar()


def reset_control_codes(token: str) -> str:
    """Remove the ASCII control codes inside of a token.

    Some LLMs (including GPT-2) use characters from 0x100 to 0x120 as
    alternatives for ASCII characters 0x00 to 0x20.

    See also https://en.wikipedia.org/wiki/%C4%A0
    """
    modified_token = ""
    for character in token:
        if 0x100 <= ord(character) <= 0x120:
            modified_token += chr(ord(character) - 0x100)
        else:
            modified_token += character

    return modified_token


def prettify_token(token: str) -> str:
    """Modify a token for printing."""

    modified_token = ""
    for character in token:
        if 0x100 <= ord(character) <= 0x120:
            modified_token += ASCII_CONTROL_CODES[ord(character) - 0x100]
        elif 0x00 <= ord(character) <= 0x20:
            modified_token += ASCII_CONTROL_CODES[ord(character)]
        else:
            modified_token += character

    return modified_token


def topk_token_probabilities(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    input_text: str,
    k: int = 10,
) -> tuple[torch.tensor, torch.tensor]:
    """Get the K most probable tokens & probabilities the model would select.

    See also `torch.topk`.

    Usage:
        >>> from transformers import AutoModelForCausalLM, AutoTokenizer
        >>> model = AutoModelForCausalLM.from_pretrained(
        ...    "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
        ... )
        >>> tokenizer = AutoTokenizer.from_pretrained(
        ...    "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
        ... )
        >>> input_text = "Here is a list of Python packages.\n\n- "
        >>> topk_token_probabilities(model, tokenizer, input_text)

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        input_text: text previously generated by LLM / inputted by user
        k: the k in "top-k"

    Returns:
        (topk_values, topk_indices): tuple of torch tensors
    """

    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits

    last_token_logits = logits[0, -1, :]
    probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)

    topk_values, topk_indices = torch.topk(probabilities, k=k)
    return topk_values, topk_indices


def balanced_tree_order(r: int, h: int) -> int:
    """Calculate the order (total number of nodes) in a balanced tree.

    See also `networkx.balanced_tree`.

    Args:
        r: Branching factor of the tree; each node will have `r` children.
        h: Height of the tree.

    Returns:
        order: number of nodes in the balanced tree.

    See also https://stackoverflow.com/a/7842866
    """
    return (r ** (h + 1) - 1) // (r - 1)


def token_decision_tree(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    input_text: str,
    k: int = 3,
    max_depth: int = 2,
    stop_strings: tuple[str, ...] | None = None,
) -> nx.DiGraph:
    """Calculate the LLM's top-k token decision tree.

    This function runs the given LLM, fully exploring the `k ** max_depth` most
    probable outcomes that the LLM could output based on the given input text.

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        input_text: text previously generated by LLM / inputted by user
        k: the k in "top-k"
        max_depth: decision tree depth

    Raises:
        ValueError: if `max_depth` is not an integer greater than 0

    Returns:
        decision_tree: NetworkX digraph decision tree
    """
    if not max_depth > 0:
        raise ValueError("max depth must be an integer greater than 0")
    if stop_strings is None:
        stop_strings = []

    decision_tree = nx.DiGraph()

    input_ids = tokenizer.encode(input_text, return_tensors="np")[-1]
    last_input_id = input_ids[-1]

    decision_tree.add_node(
        0,
        token_id=int(last_input_id),
        token=reset_control_codes(tokenizer.decode(last_input_id)),
        depth=0,
        input_text=input_text,
    )

    # We don't need to run top-k token calculations for nodes in the last layer
    for node_id in range(balanced_tree_order(k, max_depth - 1)):
        if not decision_tree.has_node(node_id):
            # Skip over nodes that don't exist from pruning
            continue

        if any(
            [sstr in decision_tree.nodes[node_id]["token"] for sstr in stop_strings]
        ):
            continue

        current_depth = decision_tree.nodes[node_id]["depth"]

        node_input_text = input_text
        if current_depth != 0:
            traversal = nx.shortest_path(decision_tree, 0, node_id)
            node_input_text += tokenizer.decode(
                [decision_tree.nodes[n]["token_id"] for n in traversal[1:]]
            )

        topk_token_probs, topk_token_ids = topk_token_probabilities(
            model, tokenizer, node_input_text, k=k
        )
        topk_tokens = tokenizer.convert_ids_to_tokens(topk_token_ids)

        current_order = decision_tree.order()
        new_node_ids = range(current_order, current_order + k)

        for k_i, new_node_id in zip(range(k), new_node_ids, strict=True):
            decision_tree.add_node(
                new_node_id,
                depth=current_depth + 1,
                token_id=topk_token_ids[k_i].item(),
                token=reset_control_codes(topk_tokens[k_i]),
            )
            decision_tree.add_edge(
                node_id, new_node_id, probability=topk_token_probs[k_i].item()
            )

    return decision_tree


def draw_decision_tree(
    decision_tree: nx.DiGraph, label_type: Literal["token", "token_id"] = "token_id"
):
    """Draw the LLM top-k token decision tree."""

    if label_type == "token_id":
        labels = {
            node_id: decision_tree.nodes[node_id][label_type]
            for node_id in decision_tree.nodes
        }
    elif label_type == "token":
        labels = {
            node_id: prettify_token(decision_tree.nodes[node_id][label_type])
            for node_id in decision_tree.nodes
        }
    else:
        msg = f"Invalid label type: {label_type}"
        raise ValueError(msg)

    edge_labels = {
        edge_id: format(decision_tree.edges[edge_id]["probability"], ".2e")
        for edge_id in decision_tree.edges
    }
    layout = nx.multipartite_layout(decision_tree, subset_key="depth")
    nx.draw(decision_tree, pos=layout, with_labels=True, labels=labels)
    nx.draw_networkx_edge_labels(decision_tree, pos=layout, edge_labels=edge_labels)


def predict_hallucinated_packages(
    model,
    tokenizer,
    language: str | None,
    package: str | None,
    k: int = 3,
    max_depth: int = 5,
) -> nx.DiGraph:
    if not max_depth >= 1:
        raise ValueError("max depth must be an integer greater than 1")
    if language is None:
        language = ""

    first_token_id = tokenizer.encode(package)[0]
    first_token = tokenizer.decode(first_token_id)
    input_text = f"Here is the name of a {language.title()} package: `{first_token}"

    return token_decision_tree(model, tokenizer, input_text, k, max_depth)


def package_in_vocabulary(tokenizer: AutoTokenizer, package: str) -> bool:
    """Determine whether the package's name is already in the tokenizer.

    We can assume that short and/or common package names like `numpy` are more
    trustworthy if they already exist in the tokenizer's vocabulary.

    Note that this should only be a worthwhile metric if the package actually
    is written in that language (e.g., `numpy` doesn't exist in JavaScript).
    """
    return package in tokenizer.get_vocab()


ASCII_CONTROL_CODES = [
    "[NUL]",
    "[SOH]",
    "[STX]",
    "[ETX]",
    "[EOT]",
    "[ENQ]",
    "[ACK]",
    "[BEL]",
    "[BS]",
    "\\t",  # HT
    "\\n",  # LF
    "\\v",  # VT
    "\\f",  # FF
    "\\r",  # CR
    "[SO]",
    "[SI]",
    "[DLE]",
    "[DC1]",
    "[DC2]",
    "[DC3]",
    "[DC4]",
    "[NAK]",
    "[SYN]",
    "[ETB]",
    "[CAN]",
    "[EM]",
    "[SUB]",
    "[ESC]",
    "[FS]",
    "[GS]",
    "[RS]",
    "[US]",
    "‚ê£",  # SP
]
"""List of alternative strings for printing ASCII control codes 0 to 32."""

if __name__ == "__main__":
    from transformers import AutoModelForCausalLM, AutoTokenizer

    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
    )
    max_depth = 3
    k = 5
    decision_tree = predict_hallucinated_packages(
        model, tokenizer, "python", "networkx", k, max_depth
    )
    input_text = decision_tree.nodes[0]["input_text"]

    for node_id in decision_tree.nodes:
        node_input_text = input_text
        if decision_tree.nodes[node_id]["depth"] == max_depth:
            traversal = nx.shortest_path(decision_tree, 0, node_id)
            node_input_text += tokenizer.decode(
                [decision_tree.nodes[n]["token_id"] for n in traversal[1:]]
            )
            print(node_input_text)
