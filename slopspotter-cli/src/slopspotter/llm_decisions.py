"""Tools for testing LLM decision trees."""

import torch
from transformers.utils.logging import disable_progress_bar

disable_progress_bar()


def topk_token_probabilities(
    model,
    tokenizer,
    input_text: str,
    k: int = 10,
) -> tuple[torch.tensor, torch.tensor]:
    """Get the K most probable tokens & probabilities the model would select.

    See also `torch.topk`.

    Usage:
        >>> from transformers import AutoModelForCausalLM, AutoTokenizer
        >>> model = AutoModelForCausalLM.from_pretrained(
        ...    "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
        ... )
        >>> tokenizer = AutoTokenizer.from_pretrained(
        ...    "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
        ... )
        >>> input_text = "Here is a list of Python packages.\n\n- "
        >>> topk_token_probabilities(model, tokenizer, input_text)

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        k: the k in "top-k"
        input_text: text previously generated by LLM / inputted by user

    Returns:
        (top_k_values, top_k_indices): tuple of torch tensors
    """

    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits

    last_token_logits = logits[0, -1, :]
    probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)

    # top_k_values, top_k_indices = torch.topk(probabilities, k=k)
    return torch.topk(probabilities, k=k)
