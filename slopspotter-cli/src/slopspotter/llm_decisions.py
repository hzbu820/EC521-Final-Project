"""Tools for testing LLM decision trees."""

import matplotlib.pyplot as plt
import networkx as nx
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.utils.logging import disable_progress_bar

disable_progress_bar()


def topk_token_probabilities(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    input_text: str,
    k: int = 10,
) -> tuple[torch.tensor, torch.tensor]:
    """Get the K most probable tokens & probabilities the model would select.

    See also `torch.topk`.

    Usage:
        >>> from transformers import AutoModelForCausalLM, AutoTokenizer
        >>> model = AutoModelForCausalLM.from_pretrained(
        ...    "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
        ... )
        >>> tokenizer = AutoTokenizer.from_pretrained(
        ...    "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
        ... )
        >>> input_text = "Here is a list of Python packages.\n\n- "
        >>> topk_token_probabilities(model, tokenizer, input_text)

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        k: the k in "top-k"
        input_text: text previously generated by LLM / inputted by user

    Returns:
        (topk_values, topk_indices): tuple of torch tensors
    """

    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits

    last_token_logits = logits[0, -1, :]
    probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)

    topk_values, topk_indices = torch.topk(probabilities, k=k)
    return topk_values, topk_indices


def token_decision_tree(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    input_text: str,
    k: int = 3,
    max_depth: int = 2,
):
    if not max_depth >= 1:
        raise ValueError("max depth must be an integer greater than 1")

    decision_tree = nx.balanced_tree(r=k, h=max_depth, create_using=nx.DiGraph)

    input_ids = tokenizer.encode(input_text, return_tensors="np")[-1]
    last_input_id = input_ids[-1]

    decision_tree.nodes[0]["token_id"] = int(last_input_id)
    decision_tree.nodes[0]["depth"] = 0

    for node_id in decision_tree.nodes():
        current_depth = decision_tree.nodes[node_id]["depth"]

        if current_depth == max_depth:
            traversal = nx.shortest_path(decision_tree, 0, node_id)
            continue

        node_input_text = input_text
        if current_depth != 0:
            traversal = nx.shortest_path(decision_tree, 0, node_id)
            node_input_text += tokenizer.decode(
                [decision_tree.nodes[n]["token_id"] for n in traversal[1:]]
            )

        _, topk_indices = topk_token_probabilities(
            model, tokenizer, node_input_text, k=k
        )
        successors = list(decision_tree.successors(node_id))
        for successor, topk_token in zip(successors, topk_indices, strict=True):
            decision_tree.nodes[successor]["depth"] = current_depth + 1
            decision_tree.nodes[successor]["token_id"] = topk_token.item()

    return decision_tree


def draw_decision_tree(decision_tree: nx.DiGraph):
    labels = {
        node_id: decision_tree.nodes[node_id]["token_id"]
        for node_id in decision_tree.nodes
    }
    layout = nx.multipartite_layout(decision_tree, subset_key="depth")
    nx.draw(decision_tree, pos=layout, with_labels=True, labels=labels)
    plt.show()


# model = AutoModelForCausalLM.from_pretrained("gpt2", device_map="auto")
# tokenizer = AutoTokenizer.from_pretrained("gpt2", device_map="auto")
# input_text = "The quick brown fox jumps over the lazy"

# decision_tree = token_decision_tree(model, tokenizer, input_text, k=3, max_depth=2)
# draw_decision_tree(decision_tree)
