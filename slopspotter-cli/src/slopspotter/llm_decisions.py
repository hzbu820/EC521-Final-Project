"""Tools for testing LLM decision trees."""

from typing import Literal

import networkx as nx
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.utils.logging import disable_progress_bar

disable_progress_bar()


def prettify_token(token: str) -> str:
    """Modify a token for readability.

    Some LLMs (including GPT-2) use characters from 0x100 to 0x120 as
    alternatives for ASCII characters 0x00 to 0x20.

    See also https://en.wikipedia.org/wiki/%C4%A0
    """

    modified_token = ""
    for character in token:
        if 0x100 <= ord(character) <= 0x120:
            modified_token += ASCII_CONTROL_CODES[ord(character) - 0x100]
        else:
            modified_token += character

    return modified_token


def topk_token_probabilities(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    input_text: str,
    k: int = 10,
) -> tuple[torch.tensor, torch.tensor]:
    """Get the K most probable tokens & probabilities the model would select.

    See also `torch.topk`.

    Usage:
        >>> from transformers import AutoModelForCausalLM, AutoTokenizer
        >>> model = AutoModelForCausalLM.from_pretrained(
        ...    "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
        ... )
        >>> tokenizer = AutoTokenizer.from_pretrained(
        ...    "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
        ... )
        >>> input_text = "Here is a list of Python packages.\n\n- "
        >>> topk_token_probabilities(model, tokenizer, input_text)

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        input_text: text previously generated by LLM / inputted by user
        k: the k in "top-k"

    Returns:
        (topk_values, topk_indices): tuple of torch tensors
    """

    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits

    last_token_logits = logits[0, -1, :]
    probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)

    topk_values, topk_indices = torch.topk(probabilities, k=k)
    return topk_values, topk_indices


def balanced_tree_order(r: int, h: int) -> int:
    """Calculate the order (total number of nodes) in a balanced tree.

    See also `networkx.balanced_tree`.

    Args:
        r: Branching factor of the tree; each node will have `r` children.
        h: Height of the tree.

    Returns:
        order: number of nodes in the balanced tree.

    See also https://stackoverflow.com/a/7842866
    """
    return (r ** (h + 1) - 1) // (r - 1)


def token_decision_tree(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    input_text: str,
    k: int = 3,
    max_depth: int = 2,
) -> nx.DiGraph:
    """Calculate the LLM's top-k token decision tree.

    This function runs the given LLM, fully exploring the `k ** max_depth` most
    probable outcomes that the LLM could output based on the given input text.

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        input_text: text previously generated by LLM / inputted by user
        k: the k in "top-k"
        max_depth: decision tree depth

    Raises:
        ValueError: if `max_depth` is not an integer greater than 0

    Returns:
        decision_tree: NetworkX digraph decision tree
    """
    if not max_depth > 0:
        raise ValueError("max depth must be an integer greater than 0")

    decision_tree = nx.balanced_tree(r=k, h=max_depth, create_using=nx.DiGraph)

    input_ids = tokenizer.encode(input_text, return_tensors="np")[-1]
    last_input_id = input_ids[-1]

    decision_tree.nodes[0]["token_id"] = int(last_input_id)
    decision_tree.nodes[0]["token"] = tokenizer.decode(last_input_id)
    decision_tree.nodes[0]["depth"] = 0
    decision_tree.nodes[0]["input_text"] = input_text

    for node_id in decision_tree.nodes():
        current_depth = decision_tree.nodes[node_id]["depth"]
        print(f"Node {node_id}\tCurrent Depth: {current_depth}")

        if current_depth >= max_depth:
            print("skipping")
            continue

        node_input_text = input_text
        if current_depth != 0:
            traversal = nx.shortest_path(decision_tree, 0, node_id)
            node_input_text += tokenizer.decode(
                [decision_tree.nodes[n]["token_id"] for n in traversal[1:]]
            )

        topk_token_probs, topk_token_ids = topk_token_probabilities(
            model, tokenizer, node_input_text, k=k
        )

        successors = list(decision_tree.successors(node_id))
        topk_tokens = tokenizer.convert_ids_to_tokens(topk_token_ids)

        for successor, token_id, token, prob in zip(
            successors,
            topk_token_ids,
            topk_tokens,
            topk_token_probs,
            strict=True,
        ):
            decision_tree.nodes[successor]["depth"] = current_depth + 1
            decision_tree.nodes[successor]["token_id"] = token_id.item()
            decision_tree.nodes[successor]["token"] = prettify_token(token)
            decision_tree.edges[(node_id, successor)]["probability"] = prob

    return decision_tree


def draw_decision_tree(
    decision_tree: nx.DiGraph, label_type: Literal["token", "token_id"] = "token_id"
):
    """Draw the LLM top-k token decision tree."""

    if label_type not in ["token", "token_id"]:
        msg = f"Invalid label type: {label_type}"
        raise ValueError(msg)

    labels = {
        node_id: decision_tree.nodes[node_id][label_type]
        for node_id in decision_tree.nodes
    }

    edge_labels = {
        edge_id: format(decision_tree.edges[edge_id]["probability"], ".2e")
        for edge_id in decision_tree.edges
    }
    layout = nx.multipartite_layout(decision_tree, subset_key="depth")
    nx.draw(decision_tree, pos=layout, with_labels=True, labels=labels)
    nx.draw_networkx_edge_labels(decision_tree, pos=layout, edge_labels=edge_labels)


def predict_hallucinated_packages(
    model,
    tokenizer,
    language: str | None,
    package: str | None,
    k: int = 3,
    max_depth: int = 5,
) -> nx.DiGraph:
    if not max_depth >= 1:
        raise ValueError("max depth must be an integer greater than 1")
    if language is None:
        language = ""

    first_token_id = tokenizer.encode(package)[0]
    first_token = tokenizer.decode(first_token_id)
    input_text = f"Here is the name of a {language.title()} package: `{first_token}"

    return token_decision_tree(model, tokenizer, input_text, k, max_depth)


def package_in_vocabulary(tokenizer: AutoTokenizer, package: str) -> bool:
    """Determine whether the package's name is already in the tokenizer.

    We can assume that short and/or common package names like `numpy` are more
    trustworthy if they already exist in the tokenizer's vocabulary.

    Note that this should only be a worthwhile metric if the package actually
    is written in that language (e.g., `numpy` doesn't exist in JavaScript).
    """
    return package in tokenizer.get_vocab()


ASCII_CONTROL_CODES = [
    "[NUL]",
    "[SOH]",
    "[STX]",
    "[ETX]",
    "[EOT]",
    "[ENQ]",
    "[ACK]",
    "[BEL]",
    "[BS]",
    "\\t",  # HT
    "\\n",  # LF
    "\\v",  # VT
    "\\f",  # FF
    "\\r",  # CR
    "[SO]",
    "[SI]",
    "[DLE]",
    "[DC1]",
    "[DC2]",
    "[DC3]",
    "[DC4]",
    "[NAK]",
    "[SYN]",
    "[ETB]",
    "[CAN]",
    "[EM]",
    "[SUB]",
    "[ESC]",
    "[FS]",
    "[GS]",
    "[RS]",
    "[US]",
    "‚ê£",  # SP
]
"""List of alternative strings for printing ASCII control codes 0 to 32."""
