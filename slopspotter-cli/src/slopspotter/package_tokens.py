"""Notebook code for generating the token decision trees for top packages."""

import json
import os
from itertools import product

import networkx as nx
import torch
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizer,
)

from slopspotter.drawing import (
    draw_decision_tree_dot,
    prettify_token,
)
from slopspotter.llm_decisions import (
    PACKAGE_INPUT_TEMPLATE,
    package_from_node_text,
    reset_control_codes,
)
from slopspotter.registries import fetch_json

LANGUAGE = "python"
TOP_PYPI_PACKAGES_LINK = (
    "https://hugovk.github.io/top-pypi-packages/top-pypi-packages.min.json"
)
PYPI_PACKAGES_JSON_FILENAME = "top-pypi-packages.json"
PACKAGE_TREE_FILENAME = "top_pypi_packages.gml"
DECISION_TREE_FILENAME = "decision_tree.gml"
ENDING_STRINGS = ["`", "`\n", "`\n\n"]


def pypi_packages_json(force: bool = False) -> dict:
    """Download or load the the PyPI Package JSON file by downloading / loading it.

    Args:
        force: if true, re-download the JSON file
    """
    # Load it if already downloaded / not forced
    if os.path.exists(PYPI_PACKAGES_JSON_FILENAME) and not force:
        print(f"Loading {PYPI_PACKAGES_JSON_FILENAME}")
        with open(PYPI_PACKAGES_JSON_FILENAME) as pypi_packages_file:
            return json.load(pypi_packages_file)

    # Download it if not already available
    top_pypi_packages = fetch_json(TOP_PYPI_PACKAGES_LINK)
    with open(PYPI_PACKAGES_JSON_FILENAME, "w") as pypi_packages_file:
        json.dump(top_pypi_packages, pypi_packages_file)
    if top_pypi_packages is not None:
        return top_pypi_packages


def pypi_package_tree(
    top_pypi_packages: dict,
    tokenizer: PreTrainedTokenizer,
    top_n: int = 100,
    force: bool = False,
) -> nx.DiGraph:
    """Make the PyPI package token-by-token decision tree.

    Args:
        top_pypi_packages: Dictionary of top PyPI packages
        tokenizer: transformers tokenizer
        top_n: Include the top N packages
        force: if true: redo the decision tree generation

    """
    if os.path.exists(PACKAGE_TREE_FILENAME) and not force:
        print(f"Loading {PACKAGE_TREE_FILENAME}")
        return nx.read_gml(PACKAGE_TREE_FILENAME, destringizer=int)

    decision_tree = nx.DiGraph()

    packages = []

    for row, end_str in product(top_pypi_packages["rows"][0:top_n], ENDING_STRINGS):
        packages.append(row["project"] + end_str)

    input_text = PACKAGE_INPUT_TEMPLATE.format(LANGUAGE.title(), "")
    input_ids = tokenizer.encode(input_text, return_tensors="np")[-1]
    last_input_id = input_ids[-1]

    decision_tree.add_node(
        0,
        token_id=int(last_input_id),
        token=reset_control_codes(tokenizer.decode(last_input_id)),
        label=reset_control_codes(tokenizer.decode(last_input_id)),
        depth=0,
        input_text=input_text,
        expected=True,
    )

    package_token_ids = [tokenizer.encode(package) for package in packages]

    for tokenized_package in package_token_ids:
        current_node_id = 0
        for depth, token_id in enumerate(tokenized_package, start=1):
            successors = list(decision_tree.successors(current_node_id))
            successor_token_ids = [
                decision_tree.nodes[successor]["token_id"] for successor in successors
            ]
            if token_id not in successor_token_ids:
                new_node_id = decision_tree.order()
                decision_tree.add_node(
                    new_node_id,
                    depth=depth + 1,
                    token_id=token_id,
                    token=prettify_token(tokenizer.decode(token_id)),
                    label=prettify_token(tokenizer.decode(token_id)),
                    expected=True,
                )
                decision_tree.add_edge(
                    current_node_id,
                    new_node_id,
                    expected=True,
                )
                current_node_id = new_node_id
            else:
                current_node_id = successors[successor_token_ids.index(token_id)]

    nx.write_gml(decision_tree, PACKAGE_TREE_FILENAME)
    draw_decision_tree_dot(decision_tree, "package_tree.png", label_type="token")
    return decision_tree


def token_probabilities(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    input_text: str,
) -> torch.Tensor:
    """Calculate the probabilities of the next token.

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        input_text: text previously generated by LLM / inputted by user
    """
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits

    last_token_logits = logits[0, -1, :]
    probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)

    return probabilities


def populate_probabilities(
    decision_tree: nx.DiGraph,
    node_id: int,
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    k: int = 3,
):
    """Populate the outgoing edges of a node in a decision tree with probabilities.

    Additionally, add the top k tokens if they're not already in the decision tree.

    Args:
        decision_tree: LLM token decision tree / package tree
        node_id: given node ID in the decision tree
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        k: the k in "top-k"



    """
    # Don't do anything if there are no successors
    if len(list(decision_tree.successors(node_id))) == 0:
        return

    # Determine the input text for this node
    node_input_text = decision_tree.nodes[0]["input_text"]
    if decision_tree.nodes[node_id]["depth"] != 0:
        traversal = nx.shortest_path(decision_tree, 0, node_id)
        node_input_text += tokenizer.decode(
            [decision_tree.nodes[n]["token_id"] for n in traversal[1:]]
        )

    # Get the token probabilities
    probabilities = token_probabilities(model, tokenizer, node_input_text)

    # Annotate edges with their probabilities
    for edge in decision_tree.out_edges(node_id):
        next_token_id = decision_tree.nodes[edge[1]]["token_id"]
        edge_probability = probabilities[next_token_id].item()
        decision_tree.edges[edge]["probability"] = edge_probability
        decision_tree.edges[edge]["label"] = edge_probability

    # Stop here if k is 0
    if k == 0:
        return

    # Get the top K most probable next tokens
    topk_values, topk_indices = torch.topk(probabilities, k=k)

    # Get successor node IDs and their token IDs
    successors = list(decision_tree.successors(node_id))
    successor_token_ids = [
        decision_tree.nodes[successor]["token_id"] for successor in successors
    ]
    # Get the current depth
    current_depth = decision_tree.nodes[node_id]["depth"]

    # Populate decision tree with new "unexpected" tokens if missing from top K tokens
    for top_value, top_index in zip(topk_values, topk_indices, strict=True):
        if top_index.item() not in successor_token_ids:
            token = reset_control_codes(tokenizer.decode(top_index))
            new_node_id = decision_tree.order()
            decision_tree.add_node(
                new_node_id,
                depth=current_depth + 1,
                token_id=top_index.item(),
                token=reset_control_codes(token),
                expected=False,
            )
            decision_tree.add_edge(
                node_id,
                new_node_id,
                probability=top_value.item(),
                expected=False,
            )


def populate_all_probabilities(
    decision_tree: nx.DiGraph,
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    k: int = 3,
):
    """Populate the probabilities for all nodes in the decision tree.

    Args:
        decision_tree: LLM token decision tree / package tree
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        k: the k in "top-k"
    """
    order = decision_tree.order()
    for node_id in tqdm(range(order)):
        populate_probabilities(decision_tree, node_id, model, tokenizer, k)
    nx.write_gml(decision_tree, "decision_tree.gml")
    draw_decision_tree_dot(decision_tree, "decision_tree.png", label_type="token")


def package_decision_tree(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    top_n: int = 10,
    force: bool = False,
):
    """Create the package LLM decision tree from the given model and tokenizer."""
    # Load it if already downloaded / not forced
    if os.path.exists(DECISION_TREE_FILENAME) and not force:
        print(f"Loading {DECISION_TREE_FILENAME}")
        return nx.read_gml(DECISION_TREE_FILENAME, destringizer=int)

    # Generate otherwise
    top_pypi_packages = pypi_packages_json(force=False)
    package_tree = pypi_package_tree(
        top_pypi_packages, tokenizer, top_n=top_n, force=force
    )
    populate_all_probabilities(package_tree, model, tokenizer)
    nx.write_gml(package_tree, DECISION_TREE_FILENAME)
    return package_tree


if __name__ == "__main__":
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
    )

    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
    )

    decision_tree = package_decision_tree(model, tokenizer, top_n=500, force=False)

    hallucinated_packages = set()

    for node_id in decision_tree.nodes:
        if decision_tree.nodes[node_id]["expected"]:
            continue
        node_input_text = decision_tree.nodes[0]["input_text"]
        if decision_tree.nodes[node_id]["depth"] != 0:
            traversal = nx.shortest_path(decision_tree, 0, node_id)
            node_input_text += tokenizer.decode(
                [decision_tree.nodes[n]["token_id"] for n in traversal[1:]]
            )
        print(node_id, package_from_node_text(node_input_text), sep="\t")

        hallucinated_packages.add(package_from_node_text(node_input_text))

    print(hallucinated_packages)
