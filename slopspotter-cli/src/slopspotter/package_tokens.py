"""Notebook code for generating the token decision trees for top packages."""

import json
from itertools import product

import networkx as nx
import torch
from joblib import Memory
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizer,
)

from slopspotter.drawing import (
    prettify_token,
)
from slopspotter.llm_decisions import (
    PACKAGE_INPUT_TEMPLATE,
    package_from_node_text,
    reset_control_codes,
)
from slopspotter.registries import fetch_json

LANGUAGE = "PyPI Python"
TOP_PYPI_PACKAGES_LINK = (
    "https://hugovk.github.io/top-pypi-packages/top-pypi-packages.min.json"
)
PYPI_PACKAGES_JSON_FILENAME = "top-pypi-packages.json"
PACKAGE_TREE_FILENAME = "top_pypi_packages.gml"
DECISION_TREE_FILENAME = "decision_tree.gml"
ENDING_STRINGS = ["`", "`\n", "`\n\n"]

CACHE_PATH = "./.data"
memory = Memory(CACHE_PATH, verbose=1)

torch.manual_seed(20251210)


@memory.cache
def pypi_packages_json() -> dict:
    """Download or load the the PyPI Package JSON file by downloading / loading it."""
    top_pypi_packages = fetch_json(TOP_PYPI_PACKAGES_LINK)
    with open(PYPI_PACKAGES_JSON_FILENAME, "w") as pypi_packages_file:
        json.dump(top_pypi_packages, pypi_packages_file)
    if top_pypi_packages is None:
        raise TimeoutError("Failed to download PyPI Package JSON")

    return top_pypi_packages


@memory.cache(ignore=["tokenizer"])
def pypi_package_tree(
    tokenizer: PreTrainedTokenizer,
    top_n: int = 100,
) -> nx.DiGraph:
    """Make the PyPI package token-by-token decision tree.

    Args:
        tokenizer: transformers tokenizer
        top_n: Include the top N packages
    """
    top_pypi_packages = pypi_packages_json()
    decision_tree = nx.DiGraph()

    packages = []

    for row, end_str in product(top_pypi_packages["rows"][0:top_n], ENDING_STRINGS):
        packages.append(row["project"] + end_str)

    input_text = PACKAGE_INPUT_TEMPLATE.format(LANGUAGE.title(), "")
    input_ids = tokenizer.encode(input_text, return_tensors="np")[-1]
    last_input_id = input_ids[-1]

    decision_tree.add_node(
        0,
        token_id=int(last_input_id),
        token=reset_control_codes(tokenizer.decode(last_input_id)),
        label=reset_control_codes(tokenizer.decode(last_input_id)),
        depth=0,
        input_text=input_text,
        expected=True,
    )

    package_token_ids = [tokenizer.encode(package) for package in packages]

    for tokenized_package in package_token_ids:
        current_node_id = 0
        for depth, token_id in enumerate(tokenized_package, start=1):
            successors = list(decision_tree.successors(current_node_id))
            successor_token_ids = [
                decision_tree.nodes[successor]["token_id"] for successor in successors
            ]
            if token_id not in successor_token_ids:
                new_node_id = decision_tree.order()
                decision_tree.add_node(
                    new_node_id,
                    depth=depth + 1,
                    token_id=token_id,
                    token=prettify_token(tokenizer.decode(token_id)),
                    label=prettify_token(tokenizer.decode(token_id)),
                    expected=True,
                )
                decision_tree.add_edge(
                    current_node_id,
                    new_node_id,
                    expected=True,
                )
                current_node_id = new_node_id
            else:
                current_node_id = successors[successor_token_ids.index(token_id)]

    return decision_tree


def token_probabilities(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    input_text: str,
) -> torch.Tensor:
    """Calculate the probabilities of the next token.

    Args:
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        input_text: text previously generated by LLM / inputted by user
    """
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits

    last_token_logits = logits[0, -1, :]
    probabilities = torch.nn.functional.softmax(last_token_logits, dim=-1)

    return probabilities


def populate_probabilities(
    decision_tree: nx.DiGraph,
    node_id: int,
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    k: int = 3,
):
    """Populate the outgoing edges of a node in a decision tree with probabilities.

    Additionally, add the top k tokens if they're not already in the decision tree.

    Args:
        decision_tree: LLM token decision tree / package tree
        node_id: given node ID in the decision tree
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        k: the k in "top-k"
    """
    # Don't do anything if there are no successors
    if len(list(decision_tree.successors(node_id))) == 0:
        return

    # Determine the input text for this node
    node_input_text = decision_tree.nodes[0]["input_text"]
    if decision_tree.nodes[node_id]["depth"] != 0:
        traversal = nx.shortest_path(decision_tree, 0, node_id)
        node_input_text += tokenizer.decode(
            [decision_tree.nodes[n]["token_id"] for n in traversal[1:]]
        )

    # Get the token probabilities
    probabilities = token_probabilities(model, tokenizer, node_input_text)

    # Annotate edges with their probabilities
    for edge in decision_tree.out_edges(node_id):
        next_token_id = decision_tree.nodes[edge[1]]["token_id"]
        edge_probability = probabilities[next_token_id].item()
        decision_tree.edges[edge]["probability"] = edge_probability
        decision_tree.edges[edge]["label"] = edge_probability

    # Stop here if k is 0
    if k == 0:
        return

    # Get the top K most probable next tokens
    topk_values, topk_indices = torch.topk(probabilities, k=k)

    # Get successor node IDs and their token IDs
    successors = list(decision_tree.successors(node_id))
    successor_token_ids = [
        decision_tree.nodes[successor]["token_id"] for successor in successors
    ]
    # Get the current depth
    current_depth = decision_tree.nodes[node_id]["depth"]

    # Populate decision tree with new "unexpected" tokens if missing from top K tokens
    for top_value, top_index in zip(topk_values, topk_indices, strict=True):
        if top_index.item() not in successor_token_ids:
            token = reset_control_codes(tokenizer.decode(top_index))
            new_node_id = decision_tree.order()
            decision_tree.add_node(
                new_node_id,
                depth=current_depth + 1,
                token_id=top_index.item(),
                token=reset_control_codes(token),
                expected=False,
            )
            decision_tree.add_edge(
                node_id,
                new_node_id,
                probability=top_value.item(),
                expected=False,
            )


def add_generated_nodes(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    decision_tree: nx.DiGraph,
    k=2,
    max_depth=4,
    stop_strings=("`", "."),
):
    """Extend the decision tree by adding an extra layer of nodes."""
    modified_nodes = 0
    order = decision_tree.order()
    for node_id in range(order):
        current_depth = decision_tree.nodes[node_id]["depth"]

        # Get successor node IDs and their token IDs
        successors = list(decision_tree.successors(node_id))
        if len(successors) >= k:
            continue

        if current_depth != 0 and any(
            [s in decision_tree.nodes[node_id]["token"] for s in stop_strings]
        ):
            # Don't propagate nodes that match a stop string
            continue

        if current_depth >= max_depth:
            continue

        # Determine the input text for this node
        node_input_text = decision_tree.nodes[0]["input_text"]
        if decision_tree.nodes[node_id]["depth"] != 0:
            traversal = nx.shortest_path(decision_tree, 0, node_id)
            node_input_text += tokenizer.decode(
                [decision_tree.nodes[n]["token_id"] for n in traversal[1:]]
            )

        # Get the token probabilities
        probabilities = token_probabilities(model, tokenizer, node_input_text)
        # Get the top K most probable next tokens
        topk_values, topk_indices = torch.topk(probabilities, k=k)

        # Get the current depth
        current_depth = decision_tree.nodes[node_id]["depth"]

        successor_token_ids = [
            decision_tree.nodes[successor]["token_id"] for successor in successors
        ]

        # Populate decision tree with new "unexpected" tokens if missing from
        # top K tokens
        for top_value, top_index in zip(topk_values, topk_indices, strict=True):
            if top_index.item() not in successor_token_ids:
                token = reset_control_codes(tokenizer.decode(top_index))
                new_node_id = decision_tree.order()
                decision_tree.add_node(
                    new_node_id,
                    depth=current_depth + 1,
                    token_id=top_index.item(),
                    token=reset_control_codes(token),
                    expected=False,
                )
                decision_tree.add_edge(
                    node_id,
                    new_node_id,
                    probability=top_value.item(),
                    expected=False,
                )
                modified_nodes += 1
    return modified_nodes


@memory.cache(ignore=["model", "tokenizer"])
def populate_all_probabilities(
    decision_tree: nx.DiGraph,
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    k: int = 3,
):
    """Populate the probabilities for all nodes in the decision tree.

    Args:
        decision_tree: LLM token decision tree / package tree
        model: transformers model for causal LM
        tokenizer: transformers tokenizer
        k: the k in "top-k"
    """
    order = decision_tree.order()
    for node_id in tqdm(range(order)):
        populate_probabilities(decision_tree, node_id, model, tokenizer, k)


@memory.cache(ignore=["model", "tokenizer"])
def extend_decision_tree(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    decision_tree: nx.DiGraph,
    k: int = 3,
    max_depth=4,
    stop_strings=("`", "."),
):
    """Fully extend the decision tree."""
    while additional_nodes := add_generated_nodes(
        model, tokenizer, decision_tree, k, max_depth, stop_strings
    ):
        print(f"adding {additional_nodes} more nodes...")
    return decision_tree


@memory.cache(ignore=["model", "tokenizer"])
def package_decision_tree(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizer,
    top_n: int = 10,
):
    """Create the package LLM decision tree from the given model and tokenizer."""
    package_tree = pypi_package_tree(tokenizer, top_n=top_n)
    populate_all_probabilities(package_tree, model, tokenizer)
    return package_tree


if __name__ == "__main__":
    my_model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
    )

    my_tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2.5-Coder-0.5B-Instruct", device_map="auto"
    )

    decision_tree = package_decision_tree(my_model, my_tokenizer, top_n=500)
    decision_tree = extend_decision_tree(
        my_model, my_tokenizer, decision_tree, k=2, max_depth=10
    )
    nx.write_gml(decision_tree, DECISION_TREE_FILENAME)

    hallucinated_packages = set()

    for node_id in decision_tree.nodes:
        if decision_tree.nodes[node_id]["expected"]:
            continue
        node_input_text = decision_tree.nodes[0]["input_text"]
        if decision_tree.nodes[node_id]["depth"] != 0:
            traversal = nx.shortest_path(decision_tree, 0, node_id)
            node_input_text += my_tokenizer.decode(
                [decision_tree.nodes[n]["token_id"] for n in traversal[1:]]
            )
        print(node_id, package_from_node_text(node_input_text), sep="\t")

        hallucinated_packages.add(package_from_node_text(node_input_text))

    print(hallucinated_packages)
